{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c4472f20-96a8-49a2-8306-dabb6e5f481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b4bd2c94-73dc-4da2-ac23-a5194597c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8995 entries, 0 to 8994\n",
      "Data columns (total 18 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   SLNO                         8995 non-null   int64  \n",
      " 1   Candidate Ref                8995 non-null   int64  \n",
      " 2   DOJ Extended                 8995 non-null   object \n",
      " 3   Duration to accept offer     8995 non-null   int64  \n",
      " 4   Notice period                8995 non-null   int64  \n",
      " 5   Offered band                 8995 non-null   object \n",
      " 6   Pecent hike expected in CTC  8995 non-null   float64\n",
      " 7   Percent hike offered in CTC  8995 non-null   float64\n",
      " 8   Percent difference CTC       8995 non-null   float64\n",
      " 9   Joining Bonus                8995 non-null   object \n",
      " 10  Candidate relocate actual    8995 non-null   object \n",
      " 11  Gender                       8995 non-null   object \n",
      " 12  Candidate Source             8995 non-null   object \n",
      " 13  Rex in Yrs                   8995 non-null   int64  \n",
      " 14  LOB                          8995 non-null   object \n",
      " 15  Location                     8995 non-null   object \n",
      " 16  Age                          8995 non-null   int64  \n",
      " 17  Status                       8995 non-null   object \n",
      "dtypes: float64(3), int64(6), object(9)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('hr_data.csv') \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dfb78b1e-05cf-4071-810c-f06ae30243a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Location_Chennai with VIF 343.56\n",
      "Removing Percent hike offered in CTC with VIF 25.11\n",
      "Removing Offered band_E2 with VIF 12.90\n",
      "\n",
      "Final VIF values:\n",
      "                               Feature       VIF\n",
      "0             Duration to accept offer  1.332659\n",
      "1                        Notice period  1.232006\n",
      "2          Pecent hike expected in CTC  1.111195\n",
      "3               Percent difference CTC  1.051662\n",
      "4                           Rex in Yrs  2.769042\n",
      "5                                  Age  1.535542\n",
      "6                     DOJ Extended_Yes  1.210812\n",
      "7                      Offered band_E1  2.118204\n",
      "8                      Offered band_E3  1.383070\n",
      "9                    Joining Bonus_Yes  1.077254\n",
      "10       Candidate relocate actual_Yes  1.063140\n",
      "11                         Gender_Male  1.032725\n",
      "12             Candidate Source_Direct  1.472196\n",
      "13  Candidate Source_Employee Referral  1.453005\n",
      "14                            LOB_BFSI  3.551174\n",
      "15                            LOB_CSMP  2.010616\n",
      "16                             LOB_EAS  1.685012\n",
      "17                             LOB_ERS  4.603709\n",
      "18                             LOB_ETS  2.173842\n",
      "19                      LOB_Healthcare  1.241763\n",
      "20                           LOB_INFRA  5.459934\n",
      "21                             LOB_MMS  1.035091\n",
      "22                  Location_Bangalore  1.456432\n",
      "23                     Location_Cochin  1.008007\n",
      "24                    Location_Gurgaon  1.056611\n",
      "25                  Location_Hyderabad  1.130027\n",
      "26                    Location_Kolkata  1.126379\n",
      "27                     Location_Mumbai  1.095385\n",
      "28                      Location_Noida  1.646765\n",
      "29                     Location_Others  1.013855\n",
      "30                       Location_Pune  1.014757\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.99      0.89      2181\n",
      "        True       0.49      0.06      0.10       518\n",
      "\n",
      "    accuracy                           0.81      2699\n",
      "   macro avg       0.65      0.52      0.50      2699\n",
      "weighted avg       0.75      0.81      0.74      2699\n",
      "\n",
      "\n",
      "Model Accuracy After Removing columns with high VIF: 80.77%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# drop irrelevant columns: 'SLNO', 'Candidate Ref'\n",
    "df = df.drop(columns=['SLNO', 'Candidate Ref'])\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "status_column = [col for col in df.columns if 'Status' in col][0] \n",
    "df['Status'] = df[status_column]  \n",
    "df = df.drop(columns=[status_column])\n",
    "X = df.drop(columns='Status')  # independent variables\n",
    "y = df['Status']               # dependent variable\n",
    "\n",
    "def calculate_vif(X):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_const = np.column_stack((np.ones(X_scaled.shape[0]), X_scaled))\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X_scaled_const, i + 1) for i in range(len(X.columns))]\n",
    "    return vif_data\n",
    "\n",
    "def remove_high_vif(X, threshold=10):\n",
    "    while True:\n",
    "        vif_data = calculate_vif(X)\n",
    "        max_vif = vif_data[\"VIF\"].max()\n",
    "        if max_vif > threshold:\n",
    "            feature_to_remove = vif_data.loc[vif_data[\"VIF\"].idxmax(), \"Feature\"]\n",
    "            print(f\"Removing {feature_to_remove} with VIF {max_vif:.2f}\")\n",
    "            X = X.drop(columns=[feature_to_remove])\n",
    "        else:\n",
    "            break\n",
    "    return X\n",
    "\n",
    "X_reduced = remove_high_vif(X)\n",
    "\n",
    "print(\"\\nFinal VIF values:\")\n",
    "print(calculate_vif(X_reduced))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=5000, solver='liblinear')\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy After Removing columns with high VIF: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b92f4486-54e9-4291-b56e-000e8133a9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Best Parameters:\n",
      "{'C': 10, 'kernel': 'rbf'}\n",
      "SVM Test Accuracy:\n",
      "0.8114\n",
      "\n",
      "kNN Best Parameters:\n",
      "{'n_neighbors': 5, 'weights': 'uniform'}\n",
      "kNN Test Accuracy:\n",
      "0.7944\n",
      "\n",
      "Random Forest Best Parameters:\n",
      "{'max_depth': 10, 'n_estimators': 100}\n",
      "Random Forest Test Accuracy:\n",
      "0.8125\n",
      "\n",
      "Best Overall Model:\n",
      "Model: Random Forest\n",
      "Best Parameters:\n",
      "{'max_depth': 10, 'n_estimators': 100}\n",
      "Best Cross-Validation Score:\n",
      "0.8220\n",
      "Test Accuracy:\n",
      "0.8125\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      1.00      0.90      2181\n",
      "        True       0.83      0.03      0.06       518\n",
      "\n",
      "    accuracy                           0.81      2699\n",
      "   macro avg       0.82      0.51      0.48      2699\n",
      "weighted avg       0.82      0.81      0.73      2699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "models = {\n",
    "    'SVM': (SVC(), {\n",
    "        'C': [1, 10],\n",
    "        'kernel': ['rbf']\n",
    "    }),\n",
    "    'kNN': (KNeighborsClassifier(), {\n",
    "        'n_neighbors': [3, 5],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    }),\n",
    "    'Random Forest': (RandomForestClassifier(), {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': [None, 10]\n",
    "    })\n",
    "}\n",
    "\n",
    "# Perform grid search for each model\n",
    "best_models = {}\n",
    "for name, (model, param_grid) in models.items():\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[name] = grid_search\n",
    "\n",
    "    print(f\"\\n{name} Best Parameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    print(f\"{name} Test Accuracy:\")\n",
    "    print(f\"{accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Identify the best overall model\n",
    "best_model_name = max(best_models, key=lambda name: best_models[name].best_score_)\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "print(\"\\nBest Overall Model:\")\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(\"Best Parameters:\")\n",
    "print(best_model.best_params_)\n",
    "print(\"Best Cross-Validation Score:\")\n",
    "print(f\"{best_model.best_score_:.4f}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Test Accuracy:\")\n",
    "print(f\"{accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b80222-ad55-4195-aa3f-f2b8c1d8cbf0",
   "metadata": {},
   "source": [
    "## Applying Standardization and removing outliers (detected using Cook's distance and Leverage value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0faea971-4ecd-405b-8008-b36a04939cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Best Parameters:\n",
      "{'C': 1, 'kernel': 'rbf'}\n",
      "SVM Test Accuracy:\n",
      "0.8541\n",
      "\n",
      "kNN Best Parameters:\n",
      "{'n_neighbors': 5, 'weights': 'uniform'}\n",
      "kNN Test Accuracy:\n",
      "0.8386\n",
      "\n",
      "Random Forest Best Parameters:\n",
      "{'max_depth': None, 'n_estimators': 100}\n",
      "Random Forest Test Accuracy:\n",
      "0.8577\n",
      "\n",
      "Best Overall Model:\n",
      "Model: Random Forest\n",
      "Best Parameters:\n",
      "{'max_depth': None, 'n_estimators': 100}\n",
      "Best Cross-Validation Score:\n",
      "0.8564\n",
      "Test Accuracy:\n",
      "0.8577\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      0.98      0.92      2149\n",
      "        True       0.59      0.13      0.21       373\n",
      "\n",
      "    accuracy                           0.86      2522\n",
      "   macro avg       0.73      0.56      0.56      2522\n",
      "weighted avg       0.83      0.86      0.82      2522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_scaled_with_const = sm.add_constant(X_scaled)\n",
    "\n",
    "# fit a model to get leverage values\n",
    "model = sm.OLS(y, X_scaled_with_const).fit()\n",
    "influence = model.get_influence()\n",
    "\n",
    "# calculate Cook's distance\n",
    "cooks_d = influence.cooks_distance[0]\n",
    "leverage = influence.hat_matrix_diag\n",
    "\n",
    "\n",
    "cooks_threshold = 4 / len(X_scaled)  # Common threshold\n",
    "leverage_threshold = 3 * (X_scaled.shape[1] / len(X_scaled))  # Leverage threshold\n",
    "\n",
    "# Identify outliers\n",
    "outliers_cooks = np.where(cooks_d > cooks_threshold)[0]\n",
    "outliers_leverage = np.where(leverage > leverage_threshold)[0]\n",
    "outliers = set(outliers_cooks) | set(outliers_leverage)\n",
    "\n",
    "# remove outliers\n",
    "X_clean = np.delete(X_scaled, list(outliers), axis=0)\n",
    "y_clean = np.delete(y, list(outliers), axis=0)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.3, random_state=42)\n",
    "\n",
    "models = {\n",
    "    'SVM': (SVC(), {\n",
    "        'C': [1, 10],\n",
    "        'kernel': ['rbf']\n",
    "    }),\n",
    "    'kNN': (KNeighborsClassifier(), {\n",
    "        'n_neighbors': [3, 5],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    }),\n",
    "    'Random Forest': (RandomForestClassifier(), {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': [None, 10]\n",
    "    })\n",
    "}\n",
    "\n",
    "# Perform grid search for each model\n",
    "best_models = {}\n",
    "for name, (model, param_grid) in models.items():\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[name] = grid_search\n",
    "\n",
    "    print(f\"\\n{name} Best Parameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    print(f\"{name} Test Accuracy:\")\n",
    "    print(f\"{accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Identify the best overall model\n",
    "best_model_name = max(best_models, key=lambda name: best_models[name].best_score_)\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "print(\"\\nBest Overall Model:\")\n",
    "print(f\"Model: {best_model_name}\")\n",
    "print(\"Best Parameters:\")\n",
    "print(best_model.best_params_)\n",
    "print(\"Best Cross-Validation Score:\")\n",
    "print(f\"{best_model.best_score_:.4f}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Test Accuracy:\")\n",
    "print(f\"{accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65431c1f-859c-4c7d-8784-9e9b43ef7f47",
   "metadata": {},
   "source": [
    "## After removing outliers and applying standardization accuracy of all models increased:\n",
    "1. **SVM from 81.14% to 85.41**\n",
    "2. **kNN from 79.44% to 83.86%**\n",
    "3. **Random Forest from 81.25% to 85.77%**\n",
    "\n",
    "### However Random Forest is still the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a2290-a89b-4bcc-b50a-cd16e05d4533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
